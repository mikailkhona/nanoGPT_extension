#If True it will launch wandb
deploy: False
# Collect experiments together. Name of log file will be wandb run id
tag: scratch
seed: 0
# default config values designed to train a gpt2 (124M) on OpenWebText
# I/O
out_dir : 'out_aug2_path_only_lol'
eval_interval : 500
log_interval : 1
eval_iters : 200
eval_only : False # if True, script exits right after the first eval
always_save_checkpoint : True # if True, always save a checkpoint after each eval
init_from : 'scratch' # 'scratch' or 'resume' or 'gpt2*'
# wandb logging
# wandb_log = False # disabled by default
wandb_project : 'owt'
wandb_run_name : 'gpt2' 
# data
dataset_train : '/home/bizon/temp-cot/Causal_graphs_chain_of_thought/path_scm_aug2/tokens_path_train.npy'
dataset_eval : '/home/bizon/temp-cot/Causal_graphs_chain_of_thought/path_scm_aug2/tokens_path_eval.npy'
gradient_accumulation_steps : 5 #40 # used to simulate larger batch sizes
batch_size : 12 # if gradient_accumulation_steps > 1, this is the micro-batch size
# model parameters
block_size : 16
n_layer : 8
n_head : 4
n_embd : 256
dropout : 0.0 # for pretraining 0 is good, for finetuning try 0.1+
bias : False # do we use bias inside LayerNorm and Linear layers?
# adamw optimizer
learning_rate : 6e-4 # max learning rate
max_iters : 600000 # total number of training iterations
weight_decay : 1e-1
beta1 : 0.9
beta2 : 0.95
grad_clip : 1.0 # clip gradients at this value, or disable if == 0.0
# learning rate decay settings
decay_lr : True # whether to decay the learning rate
warmup_iters : 200 # how many steps to warm up for
lr_decay_iters : 600000 # should be ~= max_iters per Chinchilla
min_lr : 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
# DDP settings
backend : 'nccl' # 'nccl', 'gloo', etc.
# system
device : 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
compile : False # use PyTorch 2.0 to compile the model to be faster

# Nested configs. Disable hydra logging
defaults:
  - _self_
  # - hp: cifar10  # Use this for nested file directories
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled

# Disable hydra directory structure
hydra:
  output_subdir: Null
  run:
    dir: .

  sweep:
    dir: .
    subdir: .
